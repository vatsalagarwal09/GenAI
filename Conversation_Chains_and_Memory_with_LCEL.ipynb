{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TkZ-tYIsqsAP"
   },
   "source": [
    "# Exploring Conversation Chains and Memory with LCEL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L1KvMtf54l0d"
   },
   "source": [
    "## Install OpenAI, and LangChain dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kg0y-d0bsEOa",
    "outputId": "7414f028-1ca5-4603-ba3e-198db5d7d403"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain==0.3.21 in /usr/local/lib/python3.11/dist-packages (0.3.21)\n",
      "Requirement already satisfied: langchain-core<1.0.0,>=0.3.45 in /usr/local/lib/python3.11/dist-packages (from langchain==0.3.21) (0.3.46)\n",
      "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.7 in /usr/local/lib/python3.11/dist-packages (from langchain==0.3.21) (0.3.7)\n",
      "Requirement already satisfied: langsmith<0.4,>=0.1.17 in /usr/local/lib/python3.11/dist-packages (from langchain==0.3.21) (0.3.18)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain==0.3.21) (2.10.6)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain==0.3.21) (2.0.39)\n",
      "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain==0.3.21) (2.32.3)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain==0.3.21) (6.0.2)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.45->langchain==0.3.21) (9.0.0)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.45->langchain==0.3.21) (1.33)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.45->langchain==0.3.21) (24.2)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.45->langchain==0.3.21) (4.12.2)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain==0.3.21) (0.28.1)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain==0.3.21) (3.10.15)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain==0.3.21) (1.0.0)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain==0.3.21) (0.23.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain==0.3.21) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain==0.3.21) (2.27.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain==0.3.21) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain==0.3.21) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain==0.3.21) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain==0.3.21) (2025.1.31)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain==0.3.21) (3.1.1)\n",
      "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain==0.3.21) (4.9.0)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain==0.3.21) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain==0.3.21) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.45->langchain==0.3.21) (3.0.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain==0.3.21) (1.3.1)\n",
      "Collecting langchain-openai==0.3.9\n",
      "  Downloading langchain_openai-0.3.9-py3-none-any.whl.metadata (2.3 kB)\n",
      "Requirement already satisfied: langchain-core<1.0.0,>=0.3.45 in /usr/local/lib/python3.11/dist-packages (from langchain-openai==0.3.9) (0.3.46)\n",
      "Requirement already satisfied: openai<2.0.0,>=1.66.3 in /usr/local/lib/python3.11/dist-packages (from langchain-openai==0.3.9) (1.66.5)\n",
      "Collecting tiktoken<1,>=0.7 (from langchain-openai==0.3.9)\n",
      "  Downloading tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: langsmith<0.4,>=0.1.125 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.45->langchain-openai==0.3.9) (0.3.18)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.45->langchain-openai==0.3.9) (9.0.0)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.45->langchain-openai==0.3.9) (1.33)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.45->langchain-openai==0.3.9) (6.0.2)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.45->langchain-openai==0.3.9) (24.2)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.45->langchain-openai==0.3.9) (4.12.2)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.5.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.45->langchain-openai==0.3.9) (2.10.6)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.66.3->langchain-openai==0.3.9) (4.9.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.66.3->langchain-openai==0.3.9) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.66.3->langchain-openai==0.3.9) (0.28.1)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.66.3->langchain-openai==0.3.9) (0.9.0)\n",
      "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.66.3->langchain-openai==0.3.9) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.66.3->langchain-openai==0.3.9) (4.67.1)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken<1,>=0.7->langchain-openai==0.3.9) (2024.11.6)\n",
      "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.11/dist-packages (from tiktoken<1,>=0.7->langchain-openai==0.3.9) (2.32.3)\n",
      "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.66.3->langchain-openai==0.3.9) (3.10)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.66.3->langchain-openai==0.3.9) (2025.1.31)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.66.3->langchain-openai==0.3.9) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai<2.0.0,>=1.66.3->langchain-openai==0.3.9) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.45->langchain-openai==0.3.9) (3.0.0)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core<1.0.0,>=0.3.45->langchain-openai==0.3.9) (3.10.15)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core<1.0.0,>=0.3.45->langchain-openai==0.3.9) (1.0.0)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core<1.0.0,>=0.3.45->langchain-openai==0.3.9) (0.23.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<1.0.0,>=0.3.45->langchain-openai==0.3.9) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<1.0.0,>=0.3.45->langchain-openai==0.3.9) (2.27.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken<1,>=0.7->langchain-openai==0.3.9) (3.4.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken<1,>=0.7->langchain-openai==0.3.9) (2.3.0)\n",
      "Downloading langchain_openai-0.3.9-py3-none-any.whl (60 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.9/60.9 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: tiktoken, langchain-openai\n",
      "Successfully installed langchain-openai-0.3.9 tiktoken-0.9.0\n",
      "Collecting langchain-community==0.3.19\n",
      "  Downloading langchain_community-0.3.19-py3-none-any.whl.metadata (2.4 kB)\n",
      "Requirement already satisfied: langchain-core<1.0.0,>=0.3.41 in /usr/local/lib/python3.11/dist-packages (from langchain-community==0.3.19) (0.3.46)\n",
      "Requirement already satisfied: langchain<1.0.0,>=0.3.20 in /usr/local/lib/python3.11/dist-packages (from langchain-community==0.3.19) (0.3.21)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain-community==0.3.19) (2.0.39)\n",
      "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain-community==0.3.19) (2.32.3)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community==0.3.19) (6.0.2)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community==0.3.19) (3.11.14)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community==0.3.19) (9.0.0)\n",
      "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain-community==0.3.19)\n",
      "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
      "Collecting pydantic-settings<3.0.0,>=2.4.0 (from langchain-community==0.3.19)\n",
      "  Downloading pydantic_settings-2.8.1-py3-none-any.whl.metadata (3.5 kB)\n",
      "Requirement already satisfied: langsmith<0.4,>=0.1.125 in /usr/local/lib/python3.11/dist-packages (from langchain-community==0.3.19) (0.3.18)\n",
      "Collecting httpx-sse<1.0.0,>=0.4.0 (from langchain-community==0.3.19)\n",
      "  Downloading httpx_sse-0.4.0-py3-none-any.whl.metadata (9.0 kB)\n",
      "Requirement already satisfied: numpy<3,>=1.26.2 in /usr/local/lib/python3.11/dist-packages (from langchain-community==0.3.19) (2.0.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.3.19) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.3.19) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.3.19) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.3.19) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.3.19) (6.2.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.3.19) (0.3.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.3.19) (1.18.3)\n",
      "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community==0.3.19)\n",
      "  Downloading marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
      "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community==0.3.19)\n",
      "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.7 in /usr/local/lib/python3.11/dist-packages (from langchain<1.0.0,>=0.3.20->langchain-community==0.3.19) (0.3.7)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain<1.0.0,>=0.3.20->langchain-community==0.3.19) (2.10.6)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.41->langchain-community==0.3.19) (1.33)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.41->langchain-community==0.3.19) (24.2)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.41->langchain-community==0.3.19) (4.12.2)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-community==0.3.19) (0.28.1)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-community==0.3.19) (3.10.15)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-community==0.3.19) (1.0.0)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-community==0.3.19) (0.23.0)\n",
      "Collecting python-dotenv>=0.21.0 (from pydantic-settings<3.0.0,>=2.4.0->langchain-community==0.3.19)\n",
      "  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community==0.3.19) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community==0.3.19) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community==0.3.19) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community==0.3.19) (2025.1.31)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain-community==0.3.19) (3.1.1)\n",
      "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community==0.3.19) (4.9.0)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community==0.3.19) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community==0.3.19) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.41->langchain-community==0.3.19) (3.0.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.20->langchain-community==0.3.19) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.20->langchain-community==0.3.19) (2.27.2)\n",
      "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community==0.3.19)\n",
      "  Downloading mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community==0.3.19) (1.3.1)\n",
      "Downloading langchain_community-0.3.19-py3-none-any.whl (2.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m25.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
      "Downloading httpx_sse-0.4.0-py3-none-any.whl (7.8 kB)\n",
      "Downloading pydantic_settings-2.8.1-py3-none-any.whl (30 kB)\n",
      "Downloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
      "Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
      "Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
      "Installing collected packages: python-dotenv, mypy-extensions, marshmallow, httpx-sse, typing-inspect, pydantic-settings, dataclasses-json, langchain-community\n",
      "Successfully installed dataclasses-json-0.6.7 httpx-sse-0.4.0 langchain-community-0.3.19 marshmallow-3.26.1 mypy-extensions-1.0.0 pydantic-settings-2.8.1 python-dotenv-1.0.1 typing-inspect-0.9.0\n",
      "Collecting langchain-chroma==0.1.4\n",
      "  Downloading langchain_chroma-0.1.4-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0 (from langchain-chroma==0.1.4)\n",
      "  Downloading chromadb-0.5.23-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting fastapi<1,>=0.95.2 (from langchain-chroma==0.1.4)\n",
      "  Downloading fastapi-0.115.12-py3-none-any.whl.metadata (27 kB)\n",
      "Requirement already satisfied: langchain-core<0.4,>=0.1.40 in /usr/local/lib/python3.11/dist-packages (from langchain-chroma==0.1.4) (0.3.46)\n",
      "Collecting numpy<2,>=1 (from langchain-chroma==0.1.4)\n",
      "  Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting build>=1.0.3 (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4)\n",
      "  Downloading build-1.2.2.post1-py3-none-any.whl.metadata (6.5 kB)\n",
      "Requirement already satisfied: pydantic>=1.9 in /usr/local/lib/python3.11/dist-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (2.10.6)\n",
      "Collecting chroma-hnswlib==0.7.6 (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4)\n",
      "  Downloading chroma_hnswlib-0.7.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (252 bytes)\n",
      "Collecting uvicorn>=0.18.3 (from uvicorn[standard]>=0.18.3->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4)\n",
      "  Downloading uvicorn-0.34.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting posthog>=2.4.0 (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4)\n",
      "  Downloading posthog-3.21.0-py2.py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (4.12.2)\n",
      "Collecting onnxruntime>=1.14.1 (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4)\n",
      "  Downloading onnxruntime-1.21.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (4.5 kB)\n",
      "Requirement already satisfied: opentelemetry-api>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (1.31.0)\n",
      "Collecting opentelemetry-exporter-otlp-proto-grpc>=1.2.0 (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4)\n",
      "  Downloading opentelemetry_exporter_otlp_proto_grpc-1.31.1-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting opentelemetry-instrumentation-fastapi>=0.41b0 (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4)\n",
      "  Downloading opentelemetry_instrumentation_fastapi-0.52b1-py3-none-any.whl.metadata (2.2 kB)\n",
      "Requirement already satisfied: opentelemetry-sdk>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (1.31.0)\n",
      "Collecting tokenizers<=0.20.3,>=0.13.2 (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4)\n",
      "  Downloading tokenizers-0.20.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Collecting pypika>=0.48.9 (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4)\n",
      "  Downloading PyPika-0.48.9.tar.gz (67 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
      "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
      "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "Requirement already satisfied: tqdm>=4.65.0 in /usr/local/lib/python3.11/dist-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (4.67.1)\n",
      "Collecting overrides>=7.3.1 (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4)\n",
      "  Downloading overrides-7.7.0-py3-none-any.whl.metadata (5.8 kB)\n",
      "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.11/dist-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (6.5.2)\n",
      "Requirement already satisfied: grpcio>=1.58.0 in /usr/local/lib/python3.11/dist-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (1.71.0)\n",
      "Collecting bcrypt>=4.0.1 (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4)\n",
      "  Downloading bcrypt-4.3.0-cp39-abi3-manylinux_2_34_x86_64.whl.metadata (10 kB)\n",
      "Requirement already satisfied: typer>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (0.15.2)\n",
      "Collecting kubernetes>=28.1.0 (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4)\n",
      "  Downloading kubernetes-32.0.1-py2.py3-none-any.whl.metadata (1.7 kB)\n",
      "Requirement already satisfied: tenacity>=8.2.3 in /usr/local/lib/python3.11/dist-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (9.0.0)\n",
      "Requirement already satisfied: PyYAML>=6.0.0 in /usr/local/lib/python3.11/dist-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (6.0.2)\n",
      "Collecting mmh3>=4.0.1 (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4)\n",
      "  Downloading mmh3-5.1.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (16 kB)\n",
      "Requirement already satisfied: orjson>=3.9.12 in /usr/local/lib/python3.11/dist-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (3.10.15)\n",
      "Requirement already satisfied: httpx>=0.27.0 in /usr/local/lib/python3.11/dist-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (0.28.1)\n",
      "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (13.9.4)\n",
      "Collecting starlette<0.47.0,>=0.40.0 (from fastapi<1,>=0.95.2->langchain-chroma==0.1.4)\n",
      "  Downloading starlette-0.46.1-py3-none-any.whl.metadata (6.2 kB)\n",
      "Requirement already satisfied: langsmith<0.4,>=0.1.125 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4,>=0.1.40->langchain-chroma==0.1.4) (0.3.18)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4,>=0.1.40->langchain-chroma==0.1.4) (1.33)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4,>=0.1.40->langchain-chroma==0.1.4) (24.2)\n",
      "Collecting pyproject_hooks (from build>=1.0.3->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4)\n",
      "  Downloading pyproject_hooks-1.2.0-py3-none-any.whl.metadata (1.3 kB)\n",
      "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (4.9.0)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (2025.1.31)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (1.0.7)\n",
      "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (3.10)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.27.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4,>=0.1.40->langchain-chroma==0.1.4) (3.0.0)\n",
      "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (1.17.0)\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (2.8.2)\n",
      "Requirement already satisfied: google-auth>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (2.38.0)\n",
      "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (1.8.0)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (2.32.3)\n",
      "Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (2.0.0)\n",
      "Requirement already satisfied: oauthlib>=3.2.2 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (3.2.2)\n",
      "Requirement already satisfied: urllib3>=1.24.2 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (2.3.0)\n",
      "Collecting durationpy>=0.7 (from kubernetes>=28.1.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4)\n",
      "  Downloading durationpy-0.9-py3-none-any.whl.metadata (338 bytes)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core<0.4,>=0.1.40->langchain-chroma==0.1.4) (1.0.0)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core<0.4,>=0.1.40->langchain-chroma==0.1.4) (0.23.0)\n",
      "Collecting coloredlogs (from onnxruntime>=1.14.1->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4)\n",
      "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (25.2.10)\n",
      "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (5.29.4)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (1.13.1)\n",
      "Requirement already satisfied: deprecated>=1.2.6 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-api>=1.2.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (1.2.18)\n",
      "Requirement already satisfied: importlib-metadata<8.7.0,>=6.0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-api>=1.2.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (8.6.1)\n",
      "Requirement already satisfied: googleapis-common-protos~=1.52 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (1.69.2)\n",
      "Collecting opentelemetry-exporter-otlp-proto-common==1.31.1 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4)\n",
      "  Downloading opentelemetry_exporter_otlp_proto_common-1.31.1-py3-none-any.whl.metadata (1.9 kB)\n",
      "Collecting opentelemetry-proto==1.31.1 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4)\n",
      "  Downloading opentelemetry_proto-1.31.1-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting opentelemetry-sdk>=1.2.0 (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4)\n",
      "  Downloading opentelemetry_sdk-1.31.1-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting opentelemetry-instrumentation-asgi==0.52b1 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4)\n",
      "  Downloading opentelemetry_instrumentation_asgi-0.52b1-py3-none-any.whl.metadata (2.1 kB)\n",
      "Collecting opentelemetry-instrumentation==0.52b1 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4)\n",
      "  Downloading opentelemetry_instrumentation-0.52b1-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting opentelemetry-semantic-conventions==0.52b1 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4)\n",
      "  Downloading opentelemetry_semantic_conventions-0.52b1-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting opentelemetry-util-http==0.52b1 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4)\n",
      "  Downloading opentelemetry_util_http-0.52b1-py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: wrapt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-instrumentation==0.52b1->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (1.17.2)\n",
      "Collecting asgiref~=3.0 (from opentelemetry-instrumentation-asgi==0.52b1->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4)\n",
      "  Downloading asgiref-3.8.1-py3-none-any.whl.metadata (9.3 kB)\n",
      "Collecting opentelemetry-api>=1.2.0 (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4)\n",
      "  Downloading opentelemetry_api-1.31.1-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting monotonic>=1.5 (from posthog>=2.4.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4)\n",
      "  Downloading monotonic-1.6-py2.py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting backoff>=1.10.0 (from posthog>=2.4.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4)\n",
      "  Downloading backoff-2.2.1-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: distro>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from posthog>=2.4.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (1.9.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=1.9->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=1.9->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (2.27.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (2.18.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.11/dist-packages (from tokenizers<=0.20.3,>=0.13.2->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (0.29.3)\n",
      "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer>=0.9.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (8.1.8)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer>=0.9.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (1.5.4)\n",
      "Collecting httptools>=0.6.3 (from uvicorn[standard]>=0.18.3->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4)\n",
      "  Downloading httptools-0.6.4-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\n",
      "Requirement already satisfied: python-dotenv>=0.13 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (1.0.1)\n",
      "Collecting uvloop!=0.15.0,!=0.15.1,>=0.14.0 (from uvicorn[standard]>=0.18.3->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4)\n",
      "  Downloading uvloop-0.21.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
      "Collecting watchfiles>=0.13 (from uvicorn[standard]>=0.18.3->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4)\n",
      "  Downloading watchfiles-1.0.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
      "Requirement already satisfied: websockets>=10.4 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (15.0.1)\n",
      "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx>=0.27.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (1.3.1)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (5.5.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (0.4.1)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (4.9)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<=0.20.3,>=0.13.2->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (3.18.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<=0.20.3,>=0.13.2->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (2025.3.0)\n",
      "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib-metadata<8.7.0,>=6.0->opentelemetry-api>=1.2.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (3.21.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (0.1.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->kubernetes>=28.1.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (3.4.1)\n",
      "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime>=1.14.1->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4)\n",
      "  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->onnxruntime>=1.14.1->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (1.3.0)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (0.6.1)\n",
      "Downloading langchain_chroma-0.1.4-py3-none-any.whl (10 kB)\n",
      "Downloading chromadb-0.5.23-py3-none-any.whl (628 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m628.3/628.3 kB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading chroma_hnswlib-0.7.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m46.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading fastapi-0.115.12-py3-none-any.whl (95 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m95.2/95.2 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m93.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading bcrypt-4.3.0-cp39-abi3-manylinux_2_34_x86_64.whl (284 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m284.2/284.2 kB\u001b[0m \u001b[31m28.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading build-1.2.2.post1-py3-none-any.whl (22 kB)\n",
      "Downloading kubernetes-32.0.1-py2.py3-none-any.whl (2.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m74.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading mmh3-5.1.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (101 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.6/101.6 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading onnxruntime-1.21.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (16.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.0/16.0 MB\u001b[0m \u001b[31m79.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading opentelemetry_exporter_otlp_proto_grpc-1.31.1-py3-none-any.whl (18 kB)\n",
      "Downloading opentelemetry_exporter_otlp_proto_common-1.31.1-py3-none-any.whl (18 kB)\n",
      "Downloading opentelemetry_proto-1.31.1-py3-none-any.whl (55 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.9/55.9 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading opentelemetry_instrumentation_fastapi-0.52b1-py3-none-any.whl (12 kB)\n",
      "Downloading opentelemetry_instrumentation-0.52b1-py3-none-any.whl (31 kB)\n",
      "Downloading opentelemetry_instrumentation_asgi-0.52b1-py3-none-any.whl (16 kB)\n",
      "Downloading opentelemetry_semantic_conventions-0.52b1-py3-none-any.whl (183 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.4/183.4 kB\u001b[0m \u001b[31m19.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading opentelemetry_api-1.31.1-py3-none-any.whl (65 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.2/65.2 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading opentelemetry_util_http-0.52b1-py3-none-any.whl (7.3 kB)\n",
      "Downloading opentelemetry_sdk-1.31.1-py3-none-any.whl (118 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m118.9/118.9 kB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading overrides-7.7.0-py3-none-any.whl (17 kB)\n",
      "Downloading posthog-3.21.0-py2.py3-none-any.whl (79 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.6/79.6 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading starlette-0.46.1-py3-none-any.whl (71 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tokenizers-0.20.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m26.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading uvicorn-0.34.0-py3-none-any.whl (62 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.3/62.3 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
      "Downloading durationpy-0.9-py3-none-any.whl (3.5 kB)\n",
      "Downloading httptools-0.6.4-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (459 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m459.8/459.8 kB\u001b[0m \u001b[31m35.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading monotonic-1.6-py2.py3-none-any.whl (8.2 kB)\n",
      "Downloading uvloop-0.21.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.0/4.0 MB\u001b[0m \u001b[31m98.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading watchfiles-1.0.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (452 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m452.6/452.6 kB\u001b[0m \u001b[31m33.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pyproject_hooks-1.2.0-py3-none-any.whl (10 kB)\n",
      "Downloading asgiref-3.8.1-py3-none-any.whl (23 kB)\n",
      "Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: pypika\n",
      "  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for pypika: filename=pypika-0.48.9-py2.py3-none-any.whl size=53802 sha256=c0fd1ddbf20b4db39fc5333a545910f041ae47a063e8957473cf989935285cfe\n",
      "  Stored in directory: /root/.cache/pip/wheels/a3/01/bd/4c40ceb9d5354160cb186dcc153360f4ab7eb23e2b24daf96d\n",
      "Successfully built pypika\n",
      "Installing collected packages: pypika, monotonic, durationpy, uvloop, uvicorn, pyproject_hooks, overrides, opentelemetry-util-http, opentelemetry-proto, numpy, mmh3, humanfriendly, httptools, bcrypt, backoff, asgiref, watchfiles, starlette, posthog, opentelemetry-exporter-otlp-proto-common, opentelemetry-api, coloredlogs, chroma-hnswlib, build, tokenizers, opentelemetry-semantic-conventions, onnxruntime, kubernetes, fastapi, opentelemetry-sdk, opentelemetry-instrumentation, opentelemetry-instrumentation-asgi, opentelemetry-exporter-otlp-proto-grpc, opentelemetry-instrumentation-fastapi, chromadb, langchain-chroma\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 2.0.2\n",
      "    Uninstalling numpy-2.0.2:\n",
      "      Successfully uninstalled numpy-2.0.2\n",
      "  Attempting uninstall: opentelemetry-api\n",
      "    Found existing installation: opentelemetry-api 1.31.0\n",
      "    Uninstalling opentelemetry-api-1.31.0:\n",
      "      Successfully uninstalled opentelemetry-api-1.31.0\n",
      "  Attempting uninstall: tokenizers\n",
      "    Found existing installation: tokenizers 0.21.1\n",
      "    Uninstalling tokenizers-0.21.1:\n",
      "      Successfully uninstalled tokenizers-0.21.1\n",
      "  Attempting uninstall: opentelemetry-semantic-conventions\n",
      "    Found existing installation: opentelemetry-semantic-conventions 0.52b0\n",
      "    Uninstalling opentelemetry-semantic-conventions-0.52b0:\n",
      "      Successfully uninstalled opentelemetry-semantic-conventions-0.52b0\n",
      "  Attempting uninstall: opentelemetry-sdk\n",
      "    Found existing installation: opentelemetry-sdk 1.31.0\n",
      "    Uninstalling opentelemetry-sdk-1.31.0:\n",
      "      Successfully uninstalled opentelemetry-sdk-1.31.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "transformers 4.49.0 requires tokenizers<0.22,>=0.21, but you have tokenizers 0.20.3 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed asgiref-3.8.1 backoff-2.2.1 bcrypt-4.3.0 build-1.2.2.post1 chroma-hnswlib-0.7.6 chromadb-0.5.23 coloredlogs-15.0.1 durationpy-0.9 fastapi-0.115.12 httptools-0.6.4 humanfriendly-10.0 kubernetes-32.0.1 langchain-chroma-0.1.4 mmh3-5.1.0 monotonic-1.6 numpy-1.26.4 onnxruntime-1.21.0 opentelemetry-api-1.31.1 opentelemetry-exporter-otlp-proto-common-1.31.1 opentelemetry-exporter-otlp-proto-grpc-1.31.1 opentelemetry-instrumentation-0.52b1 opentelemetry-instrumentation-asgi-0.52b1 opentelemetry-instrumentation-fastapi-0.52b1 opentelemetry-proto-1.31.1 opentelemetry-sdk-1.31.1 opentelemetry-semantic-conventions-0.52b1 opentelemetry-util-http-0.52b1 overrides-7.7.0 posthog-3.21.0 pypika-0.48.9 pyproject_hooks-1.2.0 starlette-0.46.1 tokenizers-0.20.3 uvicorn-0.34.0 uvloop-0.21.0 watchfiles-1.0.4\n"
     ]
    }
   ],
   "source": [
    "# Updated package versions and import paths \n",
    "# Ensures consistency with changes in earlier modules\n",
    "!pip install langchain==0.3.21\n",
    "!pip install langchain-openai==0.3.9\n",
    "!pip install langchain-community==0.3.19\n",
    "!pip install langchain-chroma==0.1.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H9c37cLnSrbg"
   },
   "source": [
    "## Enter Open AI API Key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cv3JzCEx_PAd"
   },
   "outputs": [],
   "source": [
    "from getpass import getpass\n",
    "\n",
    "OPENAI_KEY = getpass('Enter Open AI API Key: ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1T0s0um5Svfa"
   },
   "source": [
    "## Setup Environment Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x1YSuHNF_lbh"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ['OPENAI_API_KEY'] = OPENAI_KEY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2oeckxFBcc0E"
   },
   "source": [
    "## Load Connection to LLM\n",
    "\n",
    "Here we create a connection to ChatGPT to use later in our chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vHa9LMOfcOCV"
   },
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "# Updated parameter name from model_name to model:\n",
    "chatgpt = ChatOpenAI(model='gpt-4o-mini', temperature=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FD3EKYm8M8gg"
   },
   "source": [
    "## Working with LangChain Chains\n",
    "\n",
    "Using an LLM in isolation is fine for simple applications, but more complex applications require chaining LLMs - either with each other or with other components. Also running on multiple data points can be done easily with chains.\n",
    "\n",
    "Chain's are the legacy interface for \"chained\" applications. We define a Chain very generically as a sequence of calls to components, which can include other chains.\n",
    "\n",
    "Here we will be using LCEL chains exclusively"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iJ4-yIpEa1lx"
   },
   "source": [
    "### The Problem with Simple LLM Chains\n",
    "\n",
    "Simple LLM Chains cannot keep a track of past conversation history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jsPXwBEYavEv"
   },
   "outputs": [],
   "source": [
    "# Updated import paths for prompt templates:\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt_txt = \"\"\"{query}\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(prompt_txt)\n",
    "\n",
    "# you can also write this as llm_chain = prompt | chatgpt\n",
    "llm_chain = (\n",
    "    prompt\n",
    "      |\n",
    "    chatgpt\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0_XdMXuca-pT",
    "outputId": "b6bb30d0-6c03-4a44-e08b-36d9825c8d90"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first four colors of a rainbow are red, orange, yellow, and green.\n"
     ]
    }
   ],
   "source": [
    "response = llm_chain.invoke({'query': 'What are the first four colors of a rainbow?'})\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_FeBWLIPbGd4",
    "outputId": "e786c728-7184-4896-ebc0-a4e6cde9b678"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It seems like your question might be missing some context. Could you please provide more details or clarify what you are referring to with \"the other 3\"? This will help me assist you better!\n"
     ]
    }
   ],
   "source": [
    "response = llm_chain.invoke({'query': 'and the other 3?'})\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ng-9uxLActTT"
   },
   "source": [
    "### Conversation Chains with LCEL\n",
    "\n",
    "LangChain Expression Language (LCEL) connects prompts, models, parsers and retrieval components using a `|` pipe operator.\n",
    "\n",
    "A conversation chain basically consists of user prompts, historical conversation memory and the LLM. The LLM uses the history memory to give more contextual answers to every new prompt or user query.\n",
    "\n",
    "Memory is very important for having a true conversation with LLMs. LangChain allows us to manage conversation memory using various constructs. The main ones we will cover include:\n",
    "\n",
    "- ConversationBufferMemory\n",
    "- ConversationBufferWindowMemory\n",
    "- ConversationSummaryMemory\n",
    "- VectorStoreRetrieverMemory\n",
    "- ChatMessageHistory\n",
    "- SQLChatMessageHistory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CCZDMCXoO0Cv"
   },
   "source": [
    "### Conversation Chains with ConversationBufferMemory\n",
    "\n",
    "This is the simplest version of in-memory storage of historical conversation messages. It is basically a buffer for storing conversation memory.\n",
    "\n",
    "Remember if you have a really long conversation, you might exceed the max token limit of the context window allowed for the LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pfB6RuGLWjDT",
    "outputId": "862401fe-2fc2-47d2-acc9-89573adb56d4"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_633820/3875846320.py:14: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationBufferMemory(return_messages=True)\n"
     ]
    }
   ],
   "source": [
    "# Updated import paths for prompt templates - using simplified paths:\n",
    "from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.schema.runnable import RunnablePassthrough, RunnableLambda\n",
    "\n",
    "SYS_PROMPT = \"\"\"Act as a helpful assistant and give brief answers\"\"\"\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", SYS_PROMPT),\n",
    "        MessagesPlaceholder(variable_name=\"history\"),\n",
    "        (\"human\", \"{query}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "memory = ConversationBufferMemory(return_messages=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "71wPNZ19Wqsu",
    "outputId": "a0fcb91f-d891-44e5-a049-1e8bd48fdf30"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': []}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# function to get historical conversation messages from the memory\n",
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Uay5-chDYCfJ",
    "outputId": "b604599d-11f2-4ef0-8ced-068e233ca8cb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lets create a function now which returns the list of messages from memory\n",
    "def get_memory_messages(query):\n",
    "    return memory.load_memory_variables(query)['history']\n",
    "\n",
    "get_memory_messages('What are the first four colors of a rainbow?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r1UnntaxW3Kf",
    "outputId": "3c373dc8-941f-4f65-dbbf-da62ba5b47f2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# testing out the function with a runnable lambda which will go into our chain\n",
    "# this returns the history but we also need to send our current query to the prompt\n",
    "RunnableLambda(get_memory_messages).invoke({'query': 'What are the first four colors of a rainbow?'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xhxKM_gWXa-r",
    "outputId": "e45c44d3-cb51-473f-e41d-485380e76ce1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': 'What are the first four colors of a rainbow?', 'history': []}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we use a runnable passthrough to pass our current query untouched\n",
    "# along with the history messages to the next step in the chain\n",
    "RunnablePassthrough.assign(\n",
    "        history=RunnableLambda(get_memory_messages)\n",
    "    ).invoke({'query': 'What are the first four colors of a rainbow?'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nnfHCMuFWtn_"
   },
   "outputs": [],
   "source": [
    "# creating our conversation chain now\n",
    "def get_memory_messages(query):\n",
    "    return memory.load_memory_variables(query)['history']\n",
    "\n",
    "conversation_chain = (\n",
    "    RunnablePassthrough.assign(\n",
    "        history=RunnableLambda(get_memory_messages)\n",
    "    ) # sends current query (input by user at runtime) and history messages to next step\n",
    "      |\n",
    "    prompt # creates prompt using the previous two variables\n",
    "      |\n",
    "    chatgpt # generates response using the prompt from previous step\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aUHqbObhW8OC",
    "outputId": "1bd79e91-10f6-4edb-fc4f-5186c23a9189"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='The first four colors of a rainbow are red, orange, yellow, and green.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 17, 'prompt_tokens': 30, 'total_tokens': 47, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_0ba0d124f1', 'finish_reason': 'stop', 'logprobs': None}, id='run-4fe5a8e0-f57b-4710-a45b-11bf6e5fb577-0', usage_metadata={'input_tokens': 30, 'output_tokens': 17, 'total_tokens': 47, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = {'query': 'What are the first four colors of a rainbow?'}\n",
    "response = conversation_chain.invoke(query)\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Zz3I1Kt3Zx2F",
    "outputId": "c86990b7-d19d-46ec-b0dd-33a33c6314c9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first four colors of a rainbow are red, orange, yellow, and green.\n"
     ]
    }
   ],
   "source": [
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DfoKQ8yjZ6zV",
    "outputId": "453c2751-3ab3-44fc-e074-9c5add3edb0a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': []}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yiBksfTgZ9z4"
   },
   "outputs": [],
   "source": [
    "memory.save_context(query, {\"output\": response.content})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M8D7hnKUaix_",
    "outputId": "7889af7f-4cec-4690-faf0-e68a68c15685"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': [HumanMessage(content='What are the first four colors of a rainbow?', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='The first four colors of a rainbow are red, orange, yellow, and green.', additional_kwargs={}, response_metadata={})]}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ra8taD1VanUS",
    "outputId": "085eb595-1263-414e-8d91-416deaa3197a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The other three colors are blue, indigo, and violet.\n"
     ]
    }
   ],
   "source": [
    "query = {'query': 'and the other 3?'}\n",
    "response = conversation_chain.invoke(query)\n",
    "memory.save_context(query, {\"output\": response.content}) # remember to save your current conversation in memory\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J4hJh9jgbnMi",
    "outputId": "bcaaac0b-3f18-48d4-b30f-c04731c5508b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': [HumanMessage(content='What are the first four colors of a rainbow?', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='The first four colors of a rainbow are red, orange, yellow, and green.', additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='and the other 3?', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='The other three colors are blue, indigo, and violet.', additional_kwargs={}, response_metadata={})]}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TFogFx9JbqFN",
    "outputId": "fd5fe2e1-bbc7-4235-fef2-f27099c17661"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- **Definition**: Artificial Intelligence (AI) refers to the simulation of human intelligence in machines programmed to think and learn, enabling them to perform tasks that typically require human cognitive functions, such as problem-solving, understanding language, and recognizing patterns.\n",
      "\n",
      "- **Applications**: AI is used in various fields, including healthcare (diagnosis and treatment recommendations), finance (fraud detection and algorithmic trading), autonomous vehicles, customer service (chatbots), and many more, enhancing efficiency and decision-making.\n"
     ]
    }
   ],
   "source": [
    "query = {'query': 'Explain AI in 2 bullet points'}\n",
    "response = conversation_chain.invoke(query)\n",
    "memory.save_context(query, {\"output\": response.content}) # remember to save your current conversation in memory\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "67wu0ct9buF8",
    "outputId": "932f6783-86d2-4ff9-d03a-cafd8e2d6848"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- **Definition**: Deep Learning is a subset of machine learning that uses neural networks with many layers (deep neural networks) to model complex patterns in large datasets, enabling machines to learn from data in a way that mimics human brain function.\n",
      "\n",
      "- **Applications**: Deep Learning is widely used in image and speech recognition, natural language processing, autonomous systems, and recommendation systems, driving advancements in technologies like virtual assistants, self-driving cars, and medical imaging analysis.\n"
     ]
    }
   ],
   "source": [
    "query = {'query': 'Now do the same for Deep Learning'}\n",
    "response = conversation_chain.invoke(query)\n",
    "memory.save_context(query, {\"output\": response.content}) # remember to save your current conversation in memory\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qG1UoqoybxLQ",
    "outputId": "2ad7fcf9-a705-4f48-f710-7c30ff18894a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "So far, we have discussed the following topics:\n",
      "\n",
      "1. The first four colors of a rainbow (red, orange, yellow, green) and the remaining three (blue, indigo, violet).\n",
      "2. A brief explanation of Artificial Intelligence (AI) in two bullet points.\n",
      "3. A brief explanation of Deep Learning in two bullet points.\n"
     ]
    }
   ],
   "source": [
    "query = {'query': 'What have we discussed so far?'}\n",
    "response = conversation_chain.invoke(query)\n",
    "memory.save_context(query, {\"output\": response.content}) # remember to save your current conversation in memory\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KH-9mqvXb0mM",
    "outputId": "d372cbec-55cf-4772-974f-8c0a87de9af4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': [HumanMessage(content='What are the first four colors of a rainbow?', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='The first four colors of a rainbow are red, orange, yellow, and green.', additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='and the other 3?', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='The other three colors are blue, indigo, and violet.', additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='Explain AI in 2 bullet points', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='- **Definition**: Artificial Intelligence (AI) refers to the simulation of human intelligence in machines programmed to think and learn, enabling them to perform tasks that typically require human cognitive functions, such as problem-solving, understanding language, and recognizing patterns.\\n\\n- **Applications**: AI is used in various fields, including healthcare (diagnosis and treatment recommendations), finance (fraud detection and algorithmic trading), autonomous vehicles, customer service (chatbots), and many more, enhancing efficiency and decision-making.', additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='Now do the same for Deep Learning', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='- **Definition**: Deep Learning is a subset of machine learning that uses neural networks with many layers (deep neural networks) to model complex patterns in large datasets, enabling machines to learn from data in a way that mimics human brain function.\\n\\n- **Applications**: Deep Learning is widely used in image and speech recognition, natural language processing, autonomous systems, and recommendation systems, driving advancements in technologies like virtual assistants, self-driving cars, and medical imaging analysis.', additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='What have we discussed so far?', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='So far, we have discussed the following topics:\\n\\n1. The first four colors of a rainbow (red, orange, yellow, green) and the remaining three (blue, indigo, violet).\\n2. A brief explanation of Artificial Intelligence (AI) in two bullet points.\\n3. A brief explanation of Deep Learning in two bullet points.', additional_kwargs={}, response_metadata={})]}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z02POB29ePic"
   },
   "source": [
    "### Conversation Chains with ConversationBufferWindowMemory\n",
    "\n",
    "If you have a really long conversation, you might exceed the max token limit of the context window allowed for the LLM when using `ConversationBufferMemory` so `ConversationBufferWindowMemory` helps in just storing the last K conversations (one conversation piece is one user message and the corresponding AI message from the LLM) and thus helps you manage token limits and costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uz6mUhHTga6l",
    "outputId": "03ba3a02-acc3-41b5-80ca-748016728c3d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_633820/96416143.py:15: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationBufferWindowMemory(return_messages=True, k=2)\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "from langchain.schema.runnable import RunnablePassthrough, RunnableLambda\n",
    "\n",
    "SYS_PROMPT = \"\"\"Act as a helpful assistant and give brief answers\"\"\"\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", SYS_PROMPT),\n",
    "        MessagesPlaceholder(variable_name=\"history\"),\n",
    "        (\"human\", \"{query}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# stores last 2 sets of human-AI conversations\n",
    "memory = ConversationBufferWindowMemory(return_messages=True, k=2)\n",
    "\n",
    "# creating our conversation chain now\n",
    "def get_memory_messages(query):\n",
    "    return memory.load_memory_variables(query)['history']\n",
    "\n",
    "conversation_chain = (\n",
    "    RunnablePassthrough.assign(\n",
    "        history=RunnableLambda(get_memory_messages)\n",
    "    ) # sends current query (input by user at runtime) and history messages to next step\n",
    "      |\n",
    "    prompt # creates prompt using the previous two variables\n",
    "      |\n",
    "    chatgpt # generates response using the prompt from previous step\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_IsESp4jtHND",
    "outputId": "6e05e348-c912-4d9e-c44b-9ba8dcecd320"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first four colors of a rainbow are red, orange, yellow, and green.\n"
     ]
    }
   ],
   "source": [
    "query = {'query': 'What are the first four colors of a rainbow?'}\n",
    "response = conversation_chain.invoke(query)\n",
    "memory.save_context(query, {\"output\": response.content}) # remember to save your current conversation in memory\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NAveTuGstPsm",
    "outputId": "30902e54-e6ee-4175-c80d-cada754fdd3a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The other three colors of a rainbow are blue, indigo, and violet.\n"
     ]
    }
   ],
   "source": [
    "query = {'query': 'and the other 3?'}\n",
    "response = conversation_chain.invoke(query)\n",
    "memory.save_context(query, {\"output\": response.content}) # remember to save your current conversation in memory\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dTLfJ_1WtWwi",
    "outputId": "98e35b65-9fd3-4f83-be3a-aa0bc0dc5eab"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- **Definition**: Artificial Intelligence (AI) refers to the simulation of human intelligence in machines programmed to think and learn, enabling them to perform tasks that typically require human cognitive functions, such as problem-solving, understanding language, and recognizing patterns.\n",
      "\n",
      "- **Applications**: AI is used in various fields, including healthcare (diagnosis and treatment recommendations), finance (fraud detection and algorithmic trading), autonomous vehicles, customer service (chatbots), and many more, enhancing efficiency and decision-making.\n"
     ]
    }
   ],
   "source": [
    "query = {'query': 'Explain AI in 2 bullet points'}\n",
    "response = conversation_chain.invoke(query)\n",
    "memory.save_context(query, {\"output\": response.content}) # remember to save your current conversation in memory\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KcHC0uWJtYda",
    "outputId": "a2a83560-1d5f-4317-da86-8acc39b0d73a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- **Definition**: Deep Learning is a subset of machine learning that uses neural networks with many layers (deep neural networks) to model complex patterns in large datasets, enabling machines to learn from data in a way that mimics human brain function.\n",
      "\n",
      "- **Applications**: Deep Learning is widely used in image and speech recognition, natural language processing, autonomous systems, and generative models, driving advancements in technologies like virtual assistants, self-driving cars, and medical imaging analysis.\n"
     ]
    }
   ],
   "source": [
    "query = {'query': 'Now do the same for Deep Learning'}\n",
    "response = conversation_chain.invoke(query)\n",
    "memory.save_context(query, {\"output\": response.content}) # remember to save your current conversation in memory\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oaxZzNrctb1n",
    "outputId": "62304317-d40c-438a-97cf-9cfe265adcc7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': [HumanMessage(content='Explain AI in 2 bullet points', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='- **Definition**: Artificial Intelligence (AI) refers to the simulation of human intelligence in machines programmed to think and learn, enabling them to perform tasks that typically require human cognitive functions, such as problem-solving, understanding language, and recognizing patterns.\\n\\n- **Applications**: AI is used in various fields, including healthcare (diagnosis and treatment recommendations), finance (fraud detection and algorithmic trading), autonomous vehicles, customer service (chatbots), and many more, enhancing efficiency and decision-making.', additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='Now do the same for Deep Learning', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='- **Definition**: Deep Learning is a subset of machine learning that uses neural networks with many layers (deep neural networks) to model complex patterns in large datasets, enabling machines to learn from data in a way that mimics human brain function.\\n\\n- **Applications**: Deep Learning is widely used in image and speech recognition, natural language processing, autonomous systems, and generative models, driving advancements in technologies like virtual assistants, self-driving cars, and medical imaging analysis.', additional_kwargs={}, response_metadata={})]}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SA_ILGM0thBN",
    "outputId": "241a69bf-f952-44b5-fa02-111a9dc221de"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "So far, we have discussed:\n",
      "\n",
      "1. A brief explanation of Artificial Intelligence (AI) in two bullet points, covering its definition and applications.\n",
      "2. A brief explanation of Deep Learning in two bullet points, outlining its definition and applications.\n"
     ]
    }
   ],
   "source": [
    "query = {'query': 'What have we discussed so far?'}\n",
    "response = conversation_chain.invoke(query)\n",
    "memory.save_context(query, {\"output\": response.content}) # remember to save your current conversation in memory\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7ROjso3qtjiC",
    "outputId": "0aa52225-23c8-44c4-c871-42ff55b1e188"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': [HumanMessage(content='Now do the same for Deep Learning', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='- **Definition**: Deep Learning is a subset of machine learning that uses neural networks with many layers (deep neural networks) to model complex patterns in large datasets, enabling machines to learn from data in a way that mimics human brain function.\\n\\n- **Applications**: Deep Learning is widely used in image and speech recognition, natural language processing, autonomous systems, and generative models, driving advancements in technologies like virtual assistants, self-driving cars, and medical imaging analysis.', additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='What have we discussed so far?', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='So far, we have discussed:\\n\\n1. A brief explanation of Artificial Intelligence (AI) in two bullet points, covering its definition and applications.\\n2. A brief explanation of Deep Learning in two bullet points, outlining its definition and applications.', additional_kwargs={}, response_metadata={})]}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PEqgmHFqwLqg"
   },
   "source": [
    "### Conversation Chains with ConversationSummaryMemory\n",
    "\n",
    "If you have a really long conversation or a lot of messages, you might exceed the max token limit of the context window allowed for the LLM when using `ConversationBufferMemory`\n",
    "\n",
    "`ConversationSummaryMemory` creates a summary of the conversation history over time. This can be useful for condensing information from the conversation messages over time.\n",
    "\n",
    "This memory is most useful for longer conversations, where keeping the past message history in the prompt verbatim would take up too many tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zsvU8Wk44uBy",
    "outputId": "664dade0-a19d-4391-f434-aeb3b117f7c2"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_633820/2640930987.py:14: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationSummaryMemory(return_messages=True, llm=chatgpt)\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain.memory import ConversationSummaryMemory\n",
    "from langchain.schema.runnable import RunnablePassthrough, RunnableLambda\n",
    "\n",
    "SYS_PROMPT = \"\"\"Act as a helpful assistant and give brief answers\"\"\"\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", SYS_PROMPT),\n",
    "        MessagesPlaceholder(variable_name=\"history\"),\n",
    "        (\"human\", \"{query}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "memory = ConversationSummaryMemory(return_messages=True, llm=chatgpt)\n",
    "# creating our conversation chain now\n",
    "def get_memory_messages(query):\n",
    "    return memory.load_memory_variables(query)['history']\n",
    "\n",
    "conversation_chain = (\n",
    "    RunnablePassthrough.assign(\n",
    "        history=RunnableLambda(get_memory_messages)\n",
    "    ) # sends current query (input by user at runtime) and history messages as a summary to next step\n",
    "      |\n",
    "    prompt # creates prompt using the previous two variables\n",
    "      |\n",
    "    chatgpt # generates response using the prompt from previous step\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5cBOUQAG5t1z",
    "outputId": "1a3f3e96-1281-4ab2-e108-8a588a482db8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- **Definition**: Artificial Intelligence (AI) refers to the simulation of human intelligence in machines programmed to think and learn, enabling them to perform tasks that typically require human cognitive functions, such as problem-solving, understanding language, and recognizing patterns.\n",
      "\n",
      "- **Applications**: AI is used in various fields, including healthcare (diagnosis and treatment recommendations), finance (fraud detection and algorithmic trading), autonomous vehicles, customer service (chatbots), and more, enhancing efficiency and decision-making.\n"
     ]
    }
   ],
   "source": [
    "query = {'query': 'Explain AI in 2 bullet points'}\n",
    "response = conversation_chain.invoke(query)\n",
    "memory.save_context(query, {\"output\": response.content}) # remember to save your current conversation in memory\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DJlLfPTx5xpx",
    "outputId": "691978c2-1389-4c82-e732-0ae1b83b9745"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Deep Learning is a subset of machine learning that uses neural networks with many layers to analyze and learn from large amounts of data, enabling complex pattern recognition.\n",
      "- It is widely used in applications such as image and speech recognition, natural language processing, and autonomous systems, significantly improving accuracy and performance in these areas.\n"
     ]
    }
   ],
   "source": [
    "query = {'query': 'Now do the same for Deep Learning'}\n",
    "response = conversation_chain.invoke(query)\n",
    "memory.save_context(query, {\"output\": response.content}) # remember to save your current conversation in memory\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "La4RXy_s50Bz",
    "outputId": "79616053-14c6-4f7f-8a56-dd3bcbc8b471"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': [SystemMessage(content='The human asks the AI to explain artificial intelligence in two bullet points. The AI defines artificial intelligence as the simulation of human intelligence in machines that can think and learn, listing applications in various fields such as healthcare, finance, autonomous vehicles, and customer service, highlighting its role in enhancing efficiency and decision-making. The human then requests a similar explanation for Deep Learning, and the AI describes it as a subset of machine learning that uses neural networks with many layers for complex pattern recognition and notes its applications in image and speech recognition, natural language processing, and autonomous systems, which significantly improve accuracy and performance.', additional_kwargs={}, response_metadata={})]}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_EAa87s156RU",
    "outputId": "6e6f152a-6e30-43f4-f68e-1db111e7003c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We discussed artificial intelligence, defining it as the simulation of human intelligence in machines, with applications in various fields like healthcare and finance. We also covered deep learning, describing it as a subset of machine learning that uses neural networks for complex pattern recognition, with applications in image and speech recognition, among others.\n"
     ]
    }
   ],
   "source": [
    "query = {'query': 'What have we discussed so far?'}\n",
    "response = conversation_chain.invoke(query)\n",
    "memory.save_context(query, {\"output\": response.content}) # remember to save your current conversation in memory\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VI6HK_dw8GA2"
   },
   "source": [
    "### Conversation Chains with VectorStoreRetrieverMemory\n",
    "\n",
    "`VectorStoreRetrieverMemory` stores historical conversation messages in a vector store and queries the top-K most \"relevant\" history messages every time it is called.\n",
    "\n",
    "This differs from most of the other Memory classes in that it doesn't explicitly track the order of interactions but retrieves history based on embedding similarity to the current question or prompt.\n",
    "\n",
    "In this case, the \"docs\" are previous conversation snippets. This can be useful to refer to relevant pieces of information that the AI was told earlier in the conversation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M8nHAP7XOGOj"
   },
   "source": [
    "#### Connect to  Open AI Embedding Models\n",
    "\n",
    "LangChain enables us to access Open AI embedding models which include the newest models: a smaller and highly efficient `text-embedding-3-small` model, and a larger and more powerful `text-embedding-3-large` model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jzrIVI2NAHC1"
   },
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "# details here: https://openai.com/blog/new-embedding-models-and-api-updates\n",
    "openai_embed_model = OpenAIEmbeddings(model='text-embedding-3-small')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T_ehzSNyHZCx"
   },
   "source": [
    "#### Create a Vector Database to store conversation history\n",
    "\n",
    "Here we use the Chroma vector DB and initialize an empty database collection to store conversation messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1FZOGTTi8LhH"
   },
   "outputs": [],
   "source": [
    "from langchain_chroma import Chroma\n",
    "\n",
    "# create empty vector DB\n",
    "chroma_db = Chroma(collection_name='history_db',\n",
    "                   embedding_function=openai_embed_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IquTZdMN8nb0",
    "outputId": "98d6055b-9d09-4062-ada5-d55a1034a51f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_633820/573889585.py:18: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = VectorStoreRetrieverMemory(retriever=retriever, return_messages=True)\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain.memory import VectorStoreRetrieverMemory\n",
    "from langchain.schema.runnable import RunnablePassthrough, RunnableLambda\n",
    "\n",
    "SYS_PROMPT = \"\"\"Act as a helpful assistant and give brief answers\"\"\"\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", SYS_PROMPT),\n",
    "        MessagesPlaceholder(variable_name=\"history\"),\n",
    "        (\"human\", \"{query}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# load 2 most similar conversation messages from vector db history for each new message \\ prompt\n",
    "# this uses cosine embedding similarity to load the top 2 similar messgages to the input prompt \\ query\n",
    "retriever = chroma_db.as_retriever(search_type=\"similarity\",\n",
    "                                   search_kwargs={\"k\": 2})\n",
    "memory = VectorStoreRetrieverMemory(retriever=retriever, return_messages=True)\n",
    "\n",
    "# creating our conversation chain now\n",
    "def get_memory_messages(query):\n",
    "    return [memory.load_memory_variables(query)['history']]\n",
    "\n",
    "conversation_chain = (\n",
    "    RunnablePassthrough.assign(\n",
    "        history=RunnableLambda(get_memory_messages)\n",
    "    ) # sends current query (input by user at runtime) and history messages to next step\n",
    "      |\n",
    "    prompt # creates prompt using the previous two variables\n",
    "      |\n",
    "    chatgpt # generates response using the prompt from previous step\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VlBEB4DIGW83",
    "outputId": "d79bb3f8-6f65-4d96-856a-d847e21d54ca"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Artificial Intelligence (AI) refers to the simulation of human intelligence in machines programmed to think and learn. It encompasses various technologies, including machine learning, natural language processing, and robotics. AI can perform tasks such as speech recognition, image analysis, and decision-making, and is used in applications ranging from virtual assistants to autonomous vehicles.\n"
     ]
    }
   ],
   "source": [
    "query = {'query': 'Tell me about AI'}\n",
    "response = conversation_chain.invoke(query)\n",
    "memory.save_context(query, {\"output\": response.content}) # remember to save your current conversation in memory\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hSeAY7pZBqlN",
    "outputId": "16519a42-38e9-4eb4-b5ca-26985bd7bde1"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of requested results 2 is greater than number of elements in index 1, updating n_results = 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deep learning is a subset of machine learning that uses neural networks with many layers (deep neural networks) to analyze and learn from large amounts of data. It excels in tasks such as image and speech recognition, natural language processing, and game playing. Deep learning models automatically extract features from raw data, making them highly effective for complex problems.\n"
     ]
    }
   ],
   "source": [
    "query = {'query': 'What about deep learning'}\n",
    "response = conversation_chain.invoke(query)\n",
    "memory.save_context(query, {\"output\": response.content}) # remember to save your current conversation in memory\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rH-gVRxXBkQc",
    "outputId": "3acd6ddf-e71b-4181-f681-972091410a13"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The fastest animal in the world is the peregrine falcon, which can reach speeds over 240 mph (386 km/h) during its hunting stoop (high-speed dive). On land, the cheetah holds the record, capable of sprinting up to 60-70 mph (97-113 km/h) in short bursts.\n"
     ]
    }
   ],
   "source": [
    "query = {'query': 'Tell me about the fastest animal in the world in 2 lines'}\n",
    "response = conversation_chain.invoke(query)\n",
    "memory.save_context(query, {\"output\": response.content}) # remember to save your current conversation in memory\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VPkUBjZYEnrh",
    "outputId": "0da50da8-a752-40d2-a75d-1d835f8381e7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The cheetah is the fastest land animal, capable of reaching speeds of 60-70 mph (97-113 km/h) in short bursts covering distances up to 1,500 feet (460 meters). Its unique adaptations, such as a lightweight body, long legs, and large nasal passages, enable it to accelerate rapidly and chase down prey.\n"
     ]
    }
   ],
   "source": [
    "query = {'query': 'What about the cheetah?'}\n",
    "response = conversation_chain.invoke(query)\n",
    "memory.save_context(query, {\"output\": response.content}) # remember to save your current conversation in memory\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f-8cs1yFG8LY"
   },
   "source": [
    "Now for a new query around machine learning even if the most recent conversation messages have been about animals, it uses the vector databases to load the last 2 historical conversations which are closest to the current question in terms of semantic similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UkMDLlqdGmhW",
    "outputId": "35724b79-8c69-4d66-bd35-1b51c3c0bf24"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query: What about deep learning\n",
      "output: Deep learning is a subset of machine learning that uses neural networks with many layers (deep neural networks) to analyze and learn from large amounts of data. It excels in tasks such as image and speech recognition, natural language processing, and game playing. Deep learning models automatically extract features from raw data, making them highly effective for complex problems.\n",
      "query: Tell me about AI\n",
      "output: Artificial Intelligence (AI) refers to the simulation of human intelligence in machines programmed to think and learn. It encompasses various technologies, including machine learning, natural language processing, and robotics. AI can perform tasks such as speech recognition, image analysis, and decision-making, and is used in applications ranging from virtual assistants to autonomous vehicles.\n"
     ]
    }
   ],
   "source": [
    "print(memory.load_memory_variables({'query': 'What about machine learning?'})['history'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1KZx6U4PGi9m",
    "outputId": "29dac4a3-7c6b-46ff-c7d9-9932f4466ba1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Machine learning is a subset of artificial intelligence that focuses on the development of algorithms that allow computers to learn from and make predictions or decisions based on data. It involves training models on datasets to identify patterns and improve performance over time without being explicitly programmed for specific tasks. Machine learning is widely used in applications like recommendation systems, fraud detection, and predictive analytics.\n"
     ]
    }
   ],
   "source": [
    "query = {'query': 'What about machine learning?'}\n",
    "response = conversation_chain.invoke(query)\n",
    "memory.save_context(query, {\"output\": response.content}) # remember to save your current conversation in memory\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bqb1PyNFPTwd"
   },
   "source": [
    "### Multi-user Conversation Chains with ChatMessageHistory\n",
    "\n",
    "The concept of `ChatHistory` refers to a class in LangChain which can be used to wrap an arbitrary chain. This `ChatHistory` will keep track of inputs and outputs of the underlying chain, and append them as messages to a message database. Future interactions will then load those messages and pass them into the chain as part of the input.\n",
    "\n",
    "The beauty of `ChatMessageHistory` is that we can store separate conversation histories per user or session which is often the need for real-world chatbots which will be accessed by many users at the same time.\n",
    "\n",
    "We use a `get_session_history` function which is expected to take in a `session_id` and return a Message History object. Everything is stored in memory. This `session_id` is used to distinguish between separate conversations, and should be passed in as part of the config when calling the new chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zNT2eLWjPWtf"
   },
   "outputs": [],
   "source": [
    "from langchain.chat_message_histories import ChatMessageHistory\n",
    "from langchain.chat_history import BaseChatMessageHistory\n",
    "from langchain.runnables.history import RunnableWithMessageHistory\n",
    "from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "# used to retrieve conversation history from memory\n",
    "# based on a specific user or session ID\n",
    "history_store = {}\n",
    "def get_session_history(session_id):\n",
    "    if session_id not in history_store:\n",
    "        history_store[session_id] = ChatMessageHistory()\n",
    "    return history_store[session_id]\n",
    "\n",
    "# prompt to load in history and current input from the user\n",
    "prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"Act as a helpful AI Assistant\"),\n",
    "        MessagesPlaceholder(variable_name=\"history\"),\n",
    "        (\"human\", \"{human_input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# create a basic LLM Chain\n",
    "llm_chain = (prompt_template\n",
    "                |\n",
    "             chatgpt)\n",
    "\n",
    "# create a conversation chain which can load memory based on specific user or session id\n",
    "conv_chain = RunnableWithMessageHistory(\n",
    "    llm_chain,\n",
    "    get_session_history,\n",
    "    input_messages_key=\"human_input\",\n",
    "    history_messages_key=\"history\",\n",
    ")\n",
    "\n",
    "# create a utility function to take in current user input prompt and their session ID\n",
    "# streams result live back to the user from the LLM\n",
    "def chat_with_llm(prompt: str, session_id: str):\n",
    "    for chunk in conv_chain.stream({\"human_input\": prompt},\n",
    "                                   {'configurable': { 'session_id': session_id}}):\n",
    "        print(chunk.content, end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LNg8YkwyvXPS"
   },
   "source": [
    "Test conversation chain for user 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-34zIxbmSSCr",
    "outputId": "9b5b55e6-0dae-453a-c41f-0a19db3c170f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sure, Bob! Here are three key points about AI:\n",
      "\n",
      "1. **Definition and Purpose**: Artificial Intelligence (AI) refers to the simulation of human intelligence in machines that are programmed to think and learn. Its primary purpose is to perform tasks that typically require human intelligence, such as problem-solving, understanding natural language, and recognizing patterns.\n",
      "\n",
      "2. **Types of AI**: AI can be categorized into two main types: Narrow AI, which is designed for specific tasks (like virtual assistants or recommendation systems), and General AI, which aims to understand and reason across a wide range of tasks, similar to human cognitive abilities (though this is still largely theoretical).\n",
      "\n",
      "3. **Applications**: AI is used in various fields, including healthcare (for diagnostics and personalized medicine), finance (for fraud detection and algorithmic trading), transportation (in autonomous vehicles), and customer service (through chatbots and virtual agents), among many others."
     ]
    }
   ],
   "source": [
    "user_id = 'bob123'\n",
    "prompt = \"Hi I am Bob, can you explain AI in 3 bullet points?\"\n",
    "chat_with_llm(prompt, user_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "trsi1b4vSwaB",
    "outputId": "040d0b6b-3193-4725-fc5d-08aeffacc71f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Of course, Bob! Here are three key points about deep learning:\n",
      "\n",
      "1. **Definition and Structure**: Deep learning is a subset of machine learning that uses neural networks with many layers (hence \"deep\") to model complex patterns in large datasets. These neural networks are inspired by the structure and function of the human brain.\n",
      "\n",
      "2. **Learning Process**: Deep learning algorithms learn from vast amounts of data through a process called training, where they adjust their internal parameters to minimize the difference between their predictions and the actual outcomes. This allows them to automatically extract features and improve their performance over time.\n",
      "\n",
      "3. **Applications**: Deep learning has led to significant advancements in various fields, including image and speech recognition, natural language processing, and game playing (like AlphaGo). It powers technologies such as facial recognition systems, language translation services, and self-driving cars."
     ]
    }
   ],
   "source": [
    "prompt = \"Now do the same for deep learning\"\n",
    "chat_with_llm(prompt, user_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4uwXCxqTSzBT",
    "outputId": "ca030823-99ef-4e63-c2b1-84562b3c648f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Certainly! Here’s a brief summary of what we’ve discussed so far in bullet points:\n",
      "\n",
      "### Artificial Intelligence (AI)\n",
      "- **Definition and Purpose**: AI simulates human intelligence in machines to perform tasks that require cognitive functions.\n",
      "- **Types of AI**: Includes Narrow AI (specific tasks) and General AI (broad cognitive abilities, mostly theoretical).\n",
      "- **Applications**: Used in healthcare, finance, transportation, and customer service, among other fields.\n",
      "\n",
      "### Deep Learning\n",
      "- **Definition and Structure**: A subset of machine learning using deep neural networks to model complex patterns in data.\n",
      "- **Learning Process**: Involves training on large datasets to automatically extract features and improve predictions.\n",
      "- **Applications**: Powers advancements in image/speech recognition, natural language processing, and technologies like facial recognition and self-driving cars."
     ]
    }
   ],
   "source": [
    "prompt = \"Discuss briefly what have we discussed so far is bullet points?\"\n",
    "chat_with_llm(prompt, user_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mbU0RS9WvaUo"
   },
   "source": [
    "Now test conversation chain for user 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7opAjYZTTi_4",
    "outputId": "81077b4c-cc00-46c6-a98c-9c4b9c59ecb0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sure! Here are two key points about LLMs (Large Language Models):\n",
      "\n",
      "- **Definition**: LLMs are advanced artificial intelligence models designed to understand, generate, and manipulate human language by processing vast amounts of text data, enabling them to perform tasks like translation, summarization, and conversation.\n",
      "\n",
      "- **Architecture**: They typically use deep learning techniques, particularly transformer architectures, which allow them to capture complex patterns and relationships in language, making them highly effective for various natural language processing applications."
     ]
    }
   ],
   "source": [
    "user_id = 'james007'\n",
    "prompt = \"Hi can you explain what is an LLM in 2 bullet points?\"\n",
    "chat_with_llm(prompt, user_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zWP5yyElULYP",
    "outputId": "9094d3db-61bd-40b9-d8f7-18534a9492ef"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got it! Here’s a refined explanation of LLMs in the context of AI:\n",
      "\n",
      "- **Large Language Models (LLMs)**: These are AI models that are trained on extensive datasets to understand and generate human-like text. They leverage deep learning techniques to predict the next word in a sentence, enabling them to perform a wide range of language-related tasks.\n",
      "\n",
      "- **Applications**: LLMs are used in various applications, including chatbots, content generation, language translation, and sentiment analysis, making them valuable tools in fields like customer service, education, and creative writing."
     ]
    }
   ],
   "source": [
    "prompt = \"Actually I meant in the context of AI?\"\n",
    "chat_with_llm(prompt, user_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "spIcYAS7Ud5q",
    "outputId": "1140a252-4161-4076-ad2e-f5321b57a10a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We discussed Large Language Models (LLMs) in the context of AI, highlighting two main points: \n",
      "\n",
      "1. LLMs are advanced AI models trained on large datasets to understand and generate human-like text, utilizing deep learning techniques.\n",
      "2. They have diverse applications, including chatbots, content generation, language translation, and sentiment analysis, making them useful in various fields."
     ]
    }
   ],
   "source": [
    "prompt = \"Summarize briefly what we have discussed so far?\"\n",
    "chat_with_llm(prompt, user_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PDOoqws52yuZ",
    "outputId": "b3e0f276-aee4-49a3-ec36-17fdfd38c21d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sure! Here’s a concise summary of our discussion in bullet points:\n",
      "\n",
      "### Summary of Discussion\n",
      "- **Artificial Intelligence (AI)**:\n",
      "  - Simulates human intelligence in machines for cognitive tasks.\n",
      "  - Includes Narrow AI (specific tasks) and General AI (theoretical broad abilities).\n",
      "  - Applications span healthcare, finance, transportation, and customer service.\n",
      "\n",
      "- **Deep Learning**:\n",
      "  - A subset of machine learning using deep neural networks to analyze complex data.\n",
      "  - Involves training on large datasets to enhance feature extraction and predictions.\n",
      "  - Applications include image/speech recognition, natural language processing, and technologies like facial recognition and autonomous vehicles."
     ]
    }
   ],
   "source": [
    "user_id = 'bob123'\n",
    "prompt = \"Discuss briefly what have we discussed so far is bullet points?\"\n",
    "chat_with_llm(prompt, user_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fm6pZinBV4Q1"
   },
   "source": [
    "### Multi-user Window-based Conversation Chains with persistence - SQLChatMessageHistory\n",
    "\n",
    "The beauty of `SQLChatMessageHistory` is that we can store separate conversation histories per user or session which is often the need for real-world chatbots which will be accessed by many users at the same time. Instead of in-memory we can store it in a SQL database which can be used to store a lot of conversations.\n",
    "\n",
    "We use a `get_session_history` function which is expected to take in a `session_id` and return a Message History object. Everything is stored in a SQL database. This `session_id` is used to distinguish between separate conversations, and should be passed in as part of the config when calling the new chain\n",
    "\n",
    "We also use a `memory_buffer_window` function to only use the top-K last historical conversations before sending it to the LLM, basically our own implementation of `ConversationBufferWindowMemory`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tLdc_EEzWSKG"
   },
   "outputs": [],
   "source": [
    "# removes the memory database file - usually not needed\n",
    "# you can run this only when you want to remove all conversation histories\n",
    "!rm memory.db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J-Qam16QUgkj"
   },
   "outputs": [],
   "source": [
    "from langchain.chat_message_histories import SQLChatMessageHistory\n",
    "from langchain.chat_history import BaseChatMessageHistory\n",
    "from langchain.runnables.history import RunnableWithMessageHistory\n",
    "from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain.runnables import RunnablePassthrough\n",
    "\n",
    "# used to retrieve conversation history from database\n",
    "# based on a specific user or session ID\n",
    "def get_session_history_db(session_id):\n",
    "    return SQLChatMessageHistory(session_id, \"sqlite:///memory.db\")\n",
    "\n",
    "# prompt to load in history and current input from the user\n",
    "prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"Act as a helpful AI Assistant\"),\n",
    "        MessagesPlaceholder(variable_name=\"history\"),\n",
    "        (\"human\", \"{human_input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# create a memory buffer window function to return the last K conversations\n",
    "def memory_buffer_window(messages, k=2):\n",
    "    return messages[-(k+1):]\n",
    "\n",
    "# create a basic LLM Chain which only sends the last K conversations per user\n",
    "llm_chain = (\n",
    "    RunnablePassthrough.assign(history=lambda x: memory_buffer_window(x[\"history\"]))\n",
    "      |\n",
    "    prompt_template\n",
    "      |\n",
    "    chatgpt\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GKepk9DBrRNS"
   },
   "outputs": [],
   "source": [
    "# create a conversation chain which can load memory based on specific user or session id\n",
    "conv_chain = RunnableWithMessageHistory(\n",
    "    llm_chain,\n",
    "    get_session_history_db,\n",
    "    input_messages_key=\"human_input\",\n",
    "    history_messages_key=\"history\",\n",
    ")\n",
    "\n",
    "# create a utility function to take in current user input prompt and their session ID\n",
    "# streams result live back to the user from the LLM\n",
    "def chat_with_llm(prompt: str, session_id: str):\n",
    "    for chunk in conv_chain.stream({\"human_input\": prompt},\n",
    "                                   {'configurable': { 'session_id': session_id}}):\n",
    "        print(chunk.content, end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gkPyZbTnxBSY"
   },
   "source": [
    "Test conversation chain for user 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ebkhHCjfWl0q",
    "outputId": "cd0a6736-10c6-469d-f806-1c0b3da4012a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/santhosh/anaconda3/lib/python3.11/site-packages/langchain_core/runnables/history.py:608: LangChainDeprecationWarning: `connection_string` was deprecated in LangChain 0.2.2 and will be removed in 1.0. Use connection instead.\n",
      "  message_history = self.get_session_history(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The fastest animal in the world is the peregrine falcon. When in a dive, it can reach speeds of over 240 miles per hour (386 kilometers per hour). If you're considering speed in level flight, the Brazilian free-tailed bat holds the record, flying at speeds of around 99 miles per hour (160 kilometers per hour). For land animals, the cheetah is the fastest, capable of running speeds up to 60-70 miles per hour (97-113 kilometers per hour) in short bursts."
     ]
    }
   ],
   "source": [
    "user_id = 'jim001'\n",
    "prompt = \"Hi can you tell me which is the fastest animal?\"\n",
    "chat_with_llm(prompt, user_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cW6On1LasjYb",
    "outputId": "d1ff5812-e290-45ef-b63a-e2d8f3b45a54"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The slowest animal is often considered to be the three-toed sloth. It moves at an average speed of about 0.24 kilometers per hour (0.15 miles per hour) when climbing trees. In water, they can swim slightly faster, but they are still quite slow compared to many other animals. Another contender for the title of slowest animal is the garden snail, which moves at a speed of about 0.03 miles per hour (0.048 kilometers per hour). Both of these animals have adapted to their slow pace as part of their survival strategies."
     ]
    }
   ],
   "source": [
    "prompt = \"what about the slowest animal?\"\n",
    "chat_with_llm(prompt, user_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gMY_wEU5soCL",
    "outputId": "b273404d-e197-45ac-d852-862cf850a4d9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The largest animal on Earth is the blue whale (*Balaenoptera musculus*). Blue whales can reach lengths of up to 100 feet (30 meters) or more and can weigh as much as 200 tons (approximately 181 metric tonnes). These magnificent marine mammals are not only the largest animals alive today but are also believed to be the largest animals to have ever existed on Earth, surpassing even the largest dinosaurs. Their immense size is supported by their diet, which primarily consists of tiny shrimp-like animals called krill."
     ]
    }
   ],
   "source": [
    "prompt = \"what about the largest animal?\"\n",
    "chat_with_llm(prompt, user_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UTQPpY8RWysk",
    "outputId": "5eb04058-1577-462c-a705-b93c75acf34f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sure! Here are the topics we've discussed so far:\n",
      "\n",
      "- The slowest animal (three-toed sloth and garden snail)\n",
      "- The largest animal (blue whale)"
     ]
    }
   ],
   "source": [
    "prompt = \"what topics have we discussed, show briefly as bullet points\"\n",
    "chat_with_llm(prompt, user_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qDLyOrajxD8i"
   },
   "source": [
    "Now test conversation chain for user 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YBe4sodJW9UL",
    "outputId": "9c65be6d-dd55-44f7-de90-28bf130e8276"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sure! Here are three simple points to explain AI to a child:\n",
      "\n",
      "1. **Smart Helpers**: AI is like a smart robot or computer that can help us with tasks, like answering questions or playing games, by learning from lots of information.\n",
      "\n",
      "2. **Learning from Experience**: Just like how you get better at something the more you practice, AI learns from examples and gets better at doing things over time.\n",
      "\n",
      "3. **Talking and Understanding**: AI can understand what we say and even talk back to us, kind of like having a conversation with a friend who knows a lot of things!"
     ]
    }
   ],
   "source": [
    "user_id = 'john005'\n",
    "prompt = \"Explain AI in 3 bullets to a child\"\n",
    "chat_with_llm(prompt, user_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ebV3Mt-vXG3U",
    "outputId": "ef41b27b-d7d8-44e6-83bc-1f25a0520b65"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Of course! Here are three simple points to explain Generative AI to a child:\n",
      "\n",
      "1. **Creative Robot**: Generative AI is like a creative robot that can make new things, like pictures, stories, or music, all by itself!\n",
      "\n",
      "2. **Learning from Examples**: It learns by looking at lots of examples, like reading many books or seeing many drawings, so it can create something new that’s similar but different.\n",
      "\n",
      "3. **Imagination in a Computer**: Just like you use your imagination to come up with fun ideas, Generative AI uses its \"imagination\" to invent cool stuff that we can enjoy!"
     ]
    }
   ],
   "source": [
    "prompt = \"Now do the same for Generative AI\"\n",
    "chat_with_llm(prompt, user_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1sTqJOawtRqb",
    "outputId": "f0871dd0-faad-4f20-fb68-287c53279d16"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sure! Here are three simple points to explain machine learning to a child:\n",
      "\n",
      "1. **Learning Like You**: Machine learning is when a computer learns from experience, just like you learn new things by practicing and trying again.\n",
      "\n",
      "2. **Finding Patterns**: It looks at lots of information to find patterns, like how you notice that the sky is blue during the day or that ice cream is cold and sweet.\n",
      "\n",
      "3. **Getting Better Over Time**: The more it practices, the better it gets at making decisions or predictions, like guessing what your favorite game is based on what you’ve played before!"
     ]
    }
   ],
   "source": [
    "prompt = \"Now do the same for machine learning\"\n",
    "chat_with_llm(prompt, user_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Riyf6VhZXLEf",
    "outputId": "2d738cc7-bb89-4ec0-ce1c-07adee254faf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sure! Here are the topics we've discussed so far:\n",
      "\n",
      "- **Generative AI**\n",
      "  - Creative robot that makes new things\n",
      "  - Learns from examples\n",
      "  - Uses imagination to invent cool stuff\n",
      "\n",
      "- **Machine Learning**\n",
      "  - Learning from experience like a person\n",
      "  - Finding patterns in information\n",
      "  - Improving over time with practice"
     ]
    }
   ],
   "source": [
    "prompt = \"what topics have we discussed, show briefly as bullet points\"\n",
    "chat_with_llm(prompt, user_id)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
