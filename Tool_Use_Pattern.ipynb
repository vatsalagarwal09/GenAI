{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM2cdTHL0spHn0n8MZxO8IQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vatsalagarwal09/GenAI/blob/main/Tool_Use_Pattern.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0mqEy3C1zgbA",
        "outputId": "e508e8fc-c13f-4837-8534-e168cd57b59c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting colorama\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl.metadata (17 kB)\n",
            "Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Installing collected packages: colorama\n",
            "Successfully installed colorama-0.4.6\n"
          ]
        }
      ],
      "source": [
        "!pip install colorama"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from getpass import getpass\n",
        "\n",
        "gemini_key = getpass(\"Enter Gemini Key\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e5mZz7kxzvnc",
        "outputId": "7e8b50d4-753d-4ca0-fcf2-37af99e8ae64"
      },
      "execution_count": 2,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter Gemini Key··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"GOOGLE_API_KEY\"] = gemini_key"
      ],
      "metadata": {
        "id": "UVpsQf3iz46v"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import google.generativeai as genai\n",
        "from pprint import pprint\n",
        "from IPython.display import display_markdown\n",
        "from google.colab import userdata"
      ],
      "metadata": {
        "id": "ixYOueKpz9JG"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Building a Tool Decorator"
      ],
      "metadata": {
        "id": "sFQIcnbd4IKO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from typing import Callable\n",
        "\n",
        "\n",
        "def get_fn_signature(fn: Callable) -> dict:\n",
        "    \"\"\"\n",
        "    Generates the signature for a given function.\n",
        "\n",
        "    Args:\n",
        "        fn (Callable): The function whose signature needs to be extracted.\n",
        "\n",
        "    Returns:\n",
        "        dict: A dictionary containing the function's name, description,\n",
        "              and parameter types.\n",
        "    \"\"\"\n",
        "    fn_signature: dict = {\n",
        "        \"name\": fn.__name__,\n",
        "        \"description\": fn.__doc__,\n",
        "        \"parameters\": {\"properties\": {}},\n",
        "    }\n",
        "    schema = {\n",
        "        k: {\"type\": v.__name__} for k, v in fn.__annotations__.items() if k != \"return\"\n",
        "    }\n",
        "    fn_signature[\"parameters\"][\"properties\"] = schema\n",
        "    return fn_signature\n",
        "\n",
        "\n",
        "def validate_arguments(tool_call: dict, tool_signature: dict) -> dict:\n",
        "    \"\"\"\n",
        "    Validates and converts arguments in the input dictionary to match the expected types.\n",
        "\n",
        "    Args:\n",
        "        tool_call (dict): A dictionary containing the arguments passed to the tool.\n",
        "        tool_signature (dict): The expected function signature and parameter types.\n",
        "\n",
        "    Returns:\n",
        "        dict: The tool call dictionary with the arguments converted to the correct types if necessary.\n",
        "    \"\"\"\n",
        "    properties = tool_signature[\"parameters\"][\"properties\"]\n",
        "\n",
        "    # TODO: This is overly simplified but enough for simple Tools.\n",
        "    type_mapping = {\n",
        "        \"int\": int,\n",
        "        \"str\": str,\n",
        "        \"bool\": bool,\n",
        "        \"float\": float,\n",
        "    }\n",
        "\n",
        "    for arg_name, arg_value in tool_call[\"arguments\"].items():\n",
        "        expected_type = properties[arg_name].get(\"type\")\n",
        "\n",
        "        if not isinstance(arg_value, type_mapping[expected_type]):\n",
        "            tool_call[\"arguments\"][arg_name] = type_mapping[expected_type](arg_value)\n",
        "\n",
        "    return tool_call\n",
        "\n",
        "\n",
        "class Tool:\n",
        "    \"\"\"\n",
        "    A class representing a tool that wraps a callable and its signature.\n",
        "\n",
        "    Attributes:\n",
        "        name (str): The name of the tool (function).\n",
        "        fn (Callable): The function that the tool represents.\n",
        "        fn_signature (str): JSON string representation of the function's signature.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, name: str, fn: Callable, fn_signature: str):\n",
        "        self.name = name\n",
        "        self.fn = fn\n",
        "        self.fn_signature = fn_signature\n",
        "\n",
        "    def __str__(self):\n",
        "        return self.fn_signature\n",
        "\n",
        "    def run(self, **kwargs):\n",
        "        \"\"\"\n",
        "        Executes the tool (function) with provided arguments.\n",
        "\n",
        "        Args:\n",
        "            **kwargs: Keyword arguments passed to the function.\n",
        "\n",
        "        Returns:\n",
        "            The result of the function call.\n",
        "        \"\"\"\n",
        "        return self.fn(**kwargs)\n",
        "\n",
        "\n",
        "def tool(fn: Callable):\n",
        "    \"\"\"\n",
        "    A decorator that wraps a function into a Tool object.\n",
        "\n",
        "    Args:\n",
        "        fn (Callable): The function to be wrapped.\n",
        "\n",
        "    Returns:\n",
        "        Tool: A Tool object containing the function, its name, and its signature.\n",
        "    \"\"\"\n",
        "\n",
        "    def wrapper():\n",
        "        fn_signature = get_fn_signature(fn)\n",
        "        return Tool(\n",
        "            name=fn_signature.get(\"name\"), fn=fn, fn_signature=json.dumps(fn_signature)\n",
        "        )\n",
        "\n",
        "    return wrapper()"
      ],
      "metadata": {
        "id": "5eAqzNor-jN-"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import requests\n",
        "\n",
        "@tool\n",
        "def hn_tool(top_n: int):\n",
        "    \"\"\"\n",
        "    Fetch the top stories from Hacker News.\n",
        "\n",
        "    This function retrieves the top `top_n` stories from Hacker News using the Hacker News API.\n",
        "    Each story contains the title, URL, score, author, and time of submission. The data is fetched\n",
        "    from the official Firebase Hacker News API, which returns story details in JSON format.\n",
        "\n",
        "    Args:\n",
        "        top_n (int): The number of top stories to retrieve.\n",
        "    \"\"\"\n",
        "    top_stories_url = 'https://hacker-news.firebaseio.com/v0/topstories.json'\n",
        "\n",
        "    try:\n",
        "        response = requests.get(top_stories_url)\n",
        "        response.raise_for_status()  # Check for HTTP errors\n",
        "\n",
        "        # Get the top story IDs\n",
        "        top_story_ids = response.json()[:top_n]\n",
        "\n",
        "        top_stories = []\n",
        "\n",
        "        # For each story ID, fetch the story details\n",
        "        for story_id in top_story_ids:\n",
        "            story_url = f'https://hacker-news.firebaseio.com/v0/item/{story_id}.json'\n",
        "            story_response = requests.get(story_url)\n",
        "            story_response.raise_for_status()  # Check for HTTP errors\n",
        "            story_data = story_response.json()\n",
        "\n",
        "            # Append the story title and URL (or other relevant info) to the list\n",
        "            top_stories.append({\n",
        "                'title': story_data.get('title', 'No title'),\n",
        "                'url': story_data.get('url', 'No URL available'),\n",
        "            })\n",
        "\n",
        "        return json.dumps(top_stories)\n",
        "\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"An error occurred: {e}\")\n",
        "        return []"
      ],
      "metadata": {
        "id": "vSSFNn4a-09p"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hn_tool"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZBj81nLx4Zl7",
        "outputId": "079e0c80-c7f0-4f31-d6db-fc089a9a4b31"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<__main__.Tool at 0x7af7a2d42610>"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "json.loads(hn_tool.fn_signature)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sRqqZOpd4cB5",
        "outputId": "5091f5d5-c6ad-4903-f7bc-15c21f285696"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'name': 'hn_tool',\n",
              " 'description': '\\n    Fetch the top stories from Hacker News.\\n\\n    This function retrieves the top `top_n` stories from Hacker News using the Hacker News API.\\n    Each story contains the title, URL, score, author, and time of submission. The data is fetched\\n    from the official Firebase Hacker News API, which returns story details in JSON format.\\n\\n    Args:\\n        top_n (int): The number of top stories to retrieve.\\n    ',\n",
              " 'parameters': {'properties': {'top_n': {'type': 'int'}}}}"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Building the Tool-Using Agent"
      ],
      "metadata": {
        "id": "63qx3BjX4fvu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "\"\"\"\n",
        "This is a collection of helper functions and methods we are going to use in\n",
        "the Agent implementation. You don't need to know the specific implementation\n",
        "of these to follow the Agent code. But, if you are curious, feel free to check\n",
        "them out.\n",
        "\"\"\"\n",
        "\n",
        "import re\n",
        "import time\n",
        "\n",
        "from colorama import Fore\n",
        "from colorama import Style\n",
        "\n",
        "from dataclasses import dataclass\n",
        "\n",
        "\n",
        "def completions_create(client, messages: list, model: str) -> str:\n",
        "    \"\"\"\n",
        "    Sends a request to the client's `completions.create` method to interact with the language model.\n",
        "\n",
        "    Args:\n",
        "        client (Gemini): The Gemini client object\n",
        "        messages (list[dict]): A list of message objects containing chat history for the model.\n",
        "        model (str): The model to use for generating tool calls and responses.\n",
        "\n",
        "    Returns:\n",
        "        str: The content of the model's response.\n",
        "    \"\"\"\n",
        "    response = model.generate_content(messages)\n",
        "    return str(response.candidates[0].content.parts[0].text)\n",
        "\n",
        "\n",
        "def build_prompt_structure(prompt: str, role: str, tag: str = \"\") -> dict:\n",
        "    \"\"\"\n",
        "    Builds a structured prompt that includes the role and content.\n",
        "\n",
        "    Args:\n",
        "        prompt (str): The actual content of the prompt.\n",
        "        role (str): The role of the speaker (e.g., user, assistant).\n",
        "\n",
        "    Returns:\n",
        "        dict: A dictionary representing the structured prompt.\n",
        "    \"\"\"\n",
        "    if tag:\n",
        "        prompt = f\"<{tag}>{prompt}</{tag}>\"\n",
        "    return {\"role\": role, \"parts\": [{\"text\": prompt}]}\n",
        "\n",
        "def update_chat_history(history: list, msg: str, role: str):\n",
        "    \"\"\"\n",
        "    Updates the chat history by appending the latest response.\n",
        "\n",
        "    Args:\n",
        "        history (list): The list representing the current chat history.\n",
        "        msg (str): The message to append.\n",
        "        role (str): The role type (e.g. 'user', 'assistant', 'system')\n",
        "    \"\"\"\n",
        "    history.append(build_prompt_structure(prompt=msg, role=role))\n",
        "\n",
        "\n",
        "class ChatHistory(list):\n",
        "    def __init__(self, messages: list | None = None, total_length: int = -1):\n",
        "        \"\"\"Initialise the queue with a fixed total length.\n",
        "\n",
        "        Args:\n",
        "            messages (list | None): A list of initial messages\n",
        "            total_length (int): The maximum number of messages the chat history can hold.\n",
        "        \"\"\"\n",
        "        if messages is None:\n",
        "            messages = []\n",
        "\n",
        "        super().__init__(messages)\n",
        "        self.total_length = total_length\n",
        "\n",
        "    def append(self, msg: str):\n",
        "        \"\"\"Add a message to the queue.\n",
        "\n",
        "        Args:\n",
        "            msg (str): The message to be added to the queue\n",
        "        \"\"\"\n",
        "        if len(self) == self.total_length:\n",
        "            self.pop(0)\n",
        "        super().append(msg)\n",
        "\n",
        "\n",
        "\n",
        "class FixedFirstChatHistory(ChatHistory):\n",
        "    def __init__(self, messages: list | None = None, total_length: int = -1):\n",
        "        \"\"\"Initialise the queue with a fixed total length.\n",
        "\n",
        "        Args:\n",
        "            messages (list | None): A list of initial messages\n",
        "            total_length (int): The maximum number of messages the chat history can hold.\n",
        "        \"\"\"\n",
        "        super().__init__(messages, total_length)\n",
        "\n",
        "    def append(self, msg: str):\n",
        "        \"\"\"Add a message to the queue. The first messaage will always stay fixed.\n",
        "\n",
        "        Args:\n",
        "            msg (str): The message to be added to the queue\n",
        "        \"\"\"\n",
        "        if len(self) == self.total_length:\n",
        "            self.pop(1)\n",
        "        super().append(msg)\n",
        "\n",
        "def fancy_print(message: str) -> None:\n",
        "    \"\"\"\n",
        "    Displays a fancy print message.\n",
        "\n",
        "    Args:\n",
        "        message (str): The message to display.\n",
        "    \"\"\"\n",
        "    print(Style.BRIGHT + Fore.CYAN + f\"\\n{'=' * 50}\")\n",
        "    print(Fore.MAGENTA + f\"{message}\")\n",
        "    print(Style.BRIGHT + Fore.CYAN + f\"{'=' * 50}\\n\")\n",
        "    time.sleep(0.5)\n",
        "\n",
        "\n",
        "def fancy_step_tracker(step: int, total_steps: int) -> None:\n",
        "    \"\"\"\n",
        "    Displays a fancy step tracker for each iteration of the generation-reflection loop.\n",
        "\n",
        "    Args:\n",
        "        step (int): The current step in the loop.\n",
        "        total_steps (int): The total number of steps in the loop.\n",
        "    \"\"\"\n",
        "    fancy_print(f\"STEP {step + 1}/{total_steps}\")\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class TagContentResult:\n",
        "    \"\"\"\n",
        "    A data class to represent the result of extracting tag content.\n",
        "\n",
        "    Attributes:\n",
        "        content (List[str]): A list of strings containing the content found between the specified tags.\n",
        "        found (bool): A flag indicating whether any content was found for the given tag.\n",
        "    \"\"\"\n",
        "\n",
        "    content: list[str]\n",
        "    found: bool\n",
        "\n",
        "\n",
        "def extract_tag_content(text: str, tag: str) -> TagContentResult:\n",
        "    \"\"\"\n",
        "    Extracts all content enclosed by specified tags (e.g., <thought>, <response>, etc.).\n",
        "\n",
        "    Parameters:\n",
        "        text (str): The input string containing multiple potential tags.\n",
        "        tag (str): The name of the tag to search for (e.g., 'thought', 'response').\n",
        "\n",
        "    Returns:\n",
        "        dict: A dictionary with the following keys:\n",
        "            - 'content' (list): A list of strings containing the content found between the specified tags.\n",
        "            - 'found' (bool): A flag indicating whether any content was found for the given tag.\n",
        "    \"\"\"\n",
        "    # Build the regex pattern dynamically to find multiple occurrences of the tag\n",
        "    tag_pattern = rf\"<{tag}>(.*?)</{tag}>\"\n",
        "\n",
        "    # Use findall to capture all content between the specified tag\n",
        "    matched_contents = re.findall(tag_pattern, text, re.DOTALL)\n",
        "\n",
        "    # Return the dataclass instance with the result\n",
        "    return TagContentResult(\n",
        "        content=[content.strip() for content in matched_contents],\n",
        "        found=bool(matched_contents),\n",
        "    )\n"
      ],
      "metadata": {
        "id": "Acz7VcZ40BHV"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import re\n",
        "\n",
        "from colorama import Fore\n",
        "\n",
        "\n",
        "TOOL_SYSTEM_PROMPT = \"\"\"\n",
        "You are a function calling AI model. You are provided with function signatures within <tools></tools> XML tags.\n",
        "You may call one or more functions to assist with the user query. Don't make assumptions about what values to plug\n",
        "into functions. Pay special attention to the properties 'types'. You should use those types as in a Python dict.\n",
        "For each function call return a json object with function name and arguments within <tool_call></tool_call>\n",
        "XML tags as follows:\n",
        "\n",
        "<tool_call>\n",
        "{\"name\": <function-name>,\"arguments\": <args-dict>,  \"id\": <monotonically-increasing-id>}\n",
        "</tool_call>\n",
        "\n",
        "Here are the available tools:\n",
        "\n",
        "<tools>\n",
        "%s\n",
        "</tools>\n",
        "\n",
        "If the user asks about something related to the tool, always provide the tool call.\n",
        "Don't say that you can't access realtime information if you have a tool to achieve that.\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "class ToolAgent:\n",
        "    \"\"\"\n",
        "    The ToolAgent class represents an agent that can interact with a language model and use tools\n",
        "    to assist with user queries. It generates function calls based on user input, validates arguments,\n",
        "    and runs the respective tools.\n",
        "\n",
        "    Attributes:\n",
        "        tools (Tool | list[Tool]): A list of tools available to the agent.\n",
        "        model (str): The model to be used for generating tool calls and responses.\n",
        "        client (Gemini): The Gemini client used to interact with the language model.\n",
        "        tools_dict (dict): A dictionary mapping tool names to their corresponding Tool objects.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        tools: Tool | list[Tool],\n",
        "        model: str = \"gemini-2.5-flash\",\n",
        "    ) -> None:\n",
        "        self.tools = tools if isinstance(tools, list) else [tools]\n",
        "        self.tools_dict = {tool.name: tool for tool in self.tools}\n",
        "\n",
        "        self.client = genai.configure(api_key=gemini_key)\n",
        "\n",
        "        self.tool_generation_model = genai.GenerativeModel(\n",
        "            model,\n",
        "            system_instruction=TOOL_SYSTEM_PROMPT % self.add_tool_signatures()\n",
        "        )\n",
        "\n",
        "        self.agent_generation_model = genai.GenerativeModel(model)\n",
        "\n",
        "\n",
        "    def add_tool_signatures(self) -> str:\n",
        "        \"\"\"\n",
        "        Collects the function signatures of all available tools.\n",
        "\n",
        "        Returns:\n",
        "            str: A concatenated string of all tool function signatures in JSON format.\n",
        "        \"\"\"\n",
        "        output = \"\".join([tool.fn_signature for tool in self.tools])\n",
        "        return output;\n",
        "\n",
        "    def process_tool_calls(self, tool_calls_content: list) -> dict:\n",
        "        \"\"\"\n",
        "        Processes each tool call, validates arguments, executes the tools, and collects results.\n",
        "\n",
        "        Args:\n",
        "            tool_calls_content (list): List of strings, each representing a tool call in JSON format.\n",
        "\n",
        "        Returns:\n",
        "            dict: A dictionary where the keys are tool call IDs and values are the results from the tools.\n",
        "        \"\"\"\n",
        "        observations = {}\n",
        "        for tool_call_str in tool_calls_content:\n",
        "            tool_call = json.loads(tool_call_str)\n",
        "            tool_name = tool_call[\"name\"]\n",
        "            tool = self.tools_dict[tool_name]\n",
        "\n",
        "            print(Fore.GREEN + f\"\\nUsing Tool: {tool_name}\")\n",
        "\n",
        "            # Validate and execute the tool call\n",
        "            validated_tool_call = validate_arguments(\n",
        "                tool_call, json.loads(tool.fn_signature)\n",
        "            )\n",
        "            print(Fore.GREEN + f\"\\nTool call dict: \\n{validated_tool_call}\")\n",
        "\n",
        "            result = tool.run(**validated_tool_call[\"arguments\"])\n",
        "            print(Fore.GREEN + f\"\\nTool result: \\n{result}\")\n",
        "\n",
        "            # Store the result using the tool call ID\n",
        "            observations[validated_tool_call[\"id\"]] = result\n",
        "\n",
        "        return observations\n",
        "\n",
        "    def run(\n",
        "        self,\n",
        "        user_msg: str,\n",
        "    ) -> str:\n",
        "        \"\"\"\n",
        "        Handles the full process of interacting with the language model and executing a tool based on user input.\n",
        "\n",
        "        Args:\n",
        "            user_msg (str): The user's message that prompts the tool agent to act.\n",
        "\n",
        "        Returns:\n",
        "            str: The final output after executing the tool and generating a response from the model.\n",
        "        \"\"\"\n",
        "        user_prompt = build_prompt_structure(prompt=user_msg, role=\"user\")\n",
        "\n",
        "        tool_chat_history = ChatHistory([user_prompt])\n",
        "        agent_chat_history = ChatHistory([user_prompt])\n",
        "\n",
        "        tool_call_response = completions_create(\n",
        "            self.client, messages=tool_chat_history, model=self.tool_generation_model\n",
        "        )\n",
        "\n",
        "        tool_calls = extract_tag_content(str(tool_call_response), \"tool_call\")\n",
        "\n",
        "        if tool_calls.found:\n",
        "            observations = self.process_tool_calls(tool_calls.content)\n",
        "            update_chat_history(\n",
        "                agent_chat_history, f'f\"Observation: {observations}\"', \"user\"\n",
        "            )\n",
        "\n",
        "        return completions_create(self.client, agent_chat_history, self.agent_generation_model)\n"
      ],
      "metadata": {
        "id": "-cfMGNU1_VoZ"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hn_tool"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hr4vG3fG4_AM",
        "outputId": "020665a9-d06c-40e6-d918-e398213067df"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<__main__.Tool at 0x7af7a3b02350>"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "[hn_tool]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p3tJ4Tym5AgD",
        "outputId": "c2ae0a64-c568-4038-df83-b375a763198a"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<__main__.Tool at 0x7af7a3b02350>]"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tool_agent = ToolAgent(tools=[hn_tool])\n",
        "print(tool_agent)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UXeTLEeq4pdD",
        "outputId": "37482408-0572-4de1-fc84-6b8b01b9cfd1"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tool signatures : {\"name\": \"hn_tool\", \"description\": \"\\n    Fetch the top stories from Hacker News.\\n\\n    This function retrieves the top `top_n` stories from Hacker News using the Hacker News API.\\n    Each story contains the title, URL, score, author, and time of submission. The data is fetched\\n    from the official Firebase Hacker News API, which returns story details in JSON format.\\n\\n    Args:\\n        top_n (int): The number of top stories to retrieve.\\n    \", \"parameters\": {\"properties\": {\"top_n\": {\"type\": \"int\"}}}}\n",
            "<__main__.ToolAgent object at 0x7af7a2d677d0>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output = tool_agent.run(user_msg=\"Tell me your name\")\n",
        "print(output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "id": "6RzpqrWj4qB0",
        "outputId": "65ac3b71-d61b-4c39-f817-524f1b3d591f"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tool_call_response  I am a large language model, trained by Google.\n",
            "Agent Chat History : [{'role': 'user', 'parts': [{'text': 'Tell me your name'}]}]\n",
            "I do not have a name. I am a large language model, trained by Google.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output = tool_agent.run(user_msg=\"Tell me the top 5 Hacker News stories\")\n",
        "print(output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 402
        },
        "id": "ovASAUDI5uoj",
        "outputId": "977f6450-5eef-4356-da76-a25f32dac7e1"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tool_call_response  <tool_call>\n",
            "{\"name\": \"hn_tool\", \"arguments\": {\"top_n\": 5}, \"id\": \"call_26K2uEw9V4B29z02i4yB6L1U\"}\n",
            "</tool_call>\n",
            "\u001b[32m\n",
            "Using Tool: hn_tool\n",
            "\u001b[32m\n",
            "Tool call dict: \n",
            "{'name': 'hn_tool', 'arguments': {'top_n': 5}, 'id': 'call_26K2uEw9V4B29z02i4yB6L1U'}\n",
            "final tool in process tool calls : {\"name\": \"hn_tool\", \"description\": \"\\n    Fetch the top stories from Hacker News.\\n\\n    This function retrieves the top `top_n` stories from Hacker News using the Hacker News API.\\n    Each story contains the title, URL, score, author, and time of submission. The data is fetched\\n    from the official Firebase Hacker News API, which returns story details in JSON format.\\n\\n    Args:\\n        top_n (int): The number of top stories to retrieve.\\n    \", \"parameters\": {\"properties\": {\"top_n\": {\"type\": \"int\"}}}}\n",
            "\u001b[32m\n",
            "Tool result: \n",
            "[{\"title\": \"Twenty Eighth International Obfuscated C Code Contest\", \"url\": \"https://www.ioccc.org/2024/index.html\"}, {\"title\": \"Helsinki records zero traffic deaths for full year\", \"url\": \"https://www.helsinkitimes.fi/finland/finland-news/domestic/27539-helsinki-records-zero-traffic-deaths-for-full-year.html\"}, {\"title\": \"Micron rolls out 276-layer SSD trio for speed, scale, and stability\", \"url\": \"https://blocksandfiles.com/2025/07/30/micron-three-276-layer-ssds/\"}, {\"title\": \"Writing a basic service for GNU Guix\", \"url\": \"https://tannerhoelzel.com/gnu-shepherd-simple-service.html\"}, {\"title\": \"C++26 Reflections adventures and compile-time UML\", \"url\": \"https://www.reachablecode.com/2025/07/31/c26-reflections-adventures-compile-time-uml/\"}]\n",
            "observations ; {'call_26K2uEw9V4B29z02i4yB6L1U': '[{\"title\": \"Twenty Eighth International Obfuscated C Code Contest\", \"url\": \"https://www.ioccc.org/2024/index.html\"}, {\"title\": \"Helsinki records zero traffic deaths for full year\", \"url\": \"https://www.helsinkitimes.fi/finland/finland-news/domestic/27539-helsinki-records-zero-traffic-deaths-for-full-year.html\"}, {\"title\": \"Micron rolls out 276-layer SSD trio for speed, scale, and stability\", \"url\": \"https://blocksandfiles.com/2025/07/30/micron-three-276-layer-ssds/\"}, {\"title\": \"Writing a basic service for GNU Guix\", \"url\": \"https://tannerhoelzel.com/gnu-shepherd-simple-service.html\"}, {\"title\": \"C++26 Reflections adventures and compile-time UML\", \"url\": \"https://www.reachablecode.com/2025/07/31/c26-reflections-adventures-compile-time-uml/\"}]'}\n",
            "Agent Chat History : [{'role': 'user', 'parts': [{'text': 'Tell me the top 5 Hacker News stories'}]}, {'role': 'user', 'parts': [{'text': 'f\"Observation: {\\'call_26K2uEw9V4B29z02i4yB6L1U\\': \\'[{\"title\": \"Twenty Eighth International Obfuscated C Code Contest\", \"url\": \"https://www.ioccc.org/2024/index.html\"}, {\"title\": \"Helsinki records zero traffic deaths for full year\", \"url\": \"https://www.helsinkitimes.fi/finland/finland-news/domestic/27539-helsinki-records-zero-traffic-deaths-for-full-year.html\"}, {\"title\": \"Micron rolls out 276-layer SSD trio for speed, scale, and stability\", \"url\": \"https://blocksandfiles.com/2025/07/30/micron-three-276-layer-ssds/\"}, {\"title\": \"Writing a basic service for GNU Guix\", \"url\": \"https://tannerhoelzel.com/gnu-shepherd-simple-service.html\"}, {\"title\": \"C++26 Reflections adventures and compile-time UML\", \"url\": \"https://www.reachablecode.com/2025/07/31/c26-reflections-adventures-compile-time-uml/\"}]\\'}\"'}]}]\n",
            "Here are the top 5 Hacker News stories:\n",
            "\n",
            "1.  **Twenty Eighth International Obfuscated C Code Contest** (https://www.ioccc.org/2024/index.html)\n",
            "2.  **Helsinki records zero traffic deaths for full year** (https://www.helsinkitimes.fi/finland/finland-news/domestic/27539-helsinki-records-zero-traffic-deaths-for-full-year.html)\n",
            "3.  **Micron rolls out 276-layer SSD trio for speed, scale, and stability** (https://blocksandfiles.com/2025/07/30/micron-three-276-layer-ssds/)\n",
            "4.  **Writing a basic service for GNU Guix** (https://tannerhoelzel.com/gnu-shepherd-simple-service.html)\n",
            "5.  **C++26 Reflections adventures and compile-time UML** (https://www.reachablecode.com/2025/07/31/c26-reflections-adventures-compile-time-uml/)\n"
          ]
        }
      ]
    }
  ]
}